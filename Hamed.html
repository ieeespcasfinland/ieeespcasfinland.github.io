<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>  2021 IEEE SPS Cycle 2 Virtual School on   Networked Federated Learning: Theory, Algorithms and Applications   28.3. - 01.04.2022</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1><br /> 2021 IEEE SPS Cycle 2 Virtual School on <br /><br /> Networked Federated Learning: Theory, Algorithms and Applications <br /><br /> 28.3. - 01.04.2022</h1>
</div>
<table class="imgtable"><tr><td>
<img src="hamed.jpg" alt="70" width="140px" height="Dr. Hamed R. Tavakoli" />&nbsp;</td>
<td align="left"><p>Dr. Hamed R. Tavakoli, Nokia Technologies<br />
<a href="https://fi.linkedin.com/in/hrtavakoli"><b>Personal Site</b></a> <br /> </p>
<p><b>Lecture</b>: Compact and Efficient Neural Networks, Steps Towards Communication Efficient Federated Learning<br /></p>
<p><b>Abstract</b>: Deep Neural Networks are gaining significant amount of attention in academia and industry. Today, we are observing the emergence NN-based solutions at various industries in specific mobile devices. To achieve this, the deep neural networks must be compact and efficient, that is they could fit into a relatively small device and require less computational power. Secondly, in industries where the services are constantly improved and updates are available to the users, we require such neural networks to be deployable at scale. This talk is about how to obtain a compact and efficient neural network and how such compactness could be used in federated learning to enhance the communication efficiency. To start, we touch base the basics of compactization of neural networks, for example, sparsification and pruning technologies. We continue with an overview of quantization techniques that are used at scale for deployment of NNs. Then, we make a bridge to communication efficiency where we will briefly discuss MPEG NNC standard as an industrial test bed. We, then, continue with the techniques that could be adapted to communication efficiency for federated learning. We will overview some of the methods from distributed learning of neural networks. Then, we delve into different communication efficient federated learning schemes. We will discuss about algorithms that are agnostic to the federation algorithms and federation specific algorithms where some assumptions are required.  <br /> </p>
<p><b>Presenter</b>: Hamed R. Tavakoli is a principal scientist at Nokia Technologies, Finland. He has been leading the neural network compression and federated learning over the past few years. Prior to joining Nokia, Hamed was with the Aalto university, where has been researching machine learning with the focus on computer vision and its intersection with natural language processing. <br /><br /> </p>
<p><b>Note</b>: The talk is based on the publicly available research that has been carried out at Nokia Technologies and contributed to MPEG NNC standardization and the vastly available techniques that are developed by academia and industry.</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-03-18 12:28:07 EET, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
